{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9733518,"sourceType":"datasetVersion","datasetId":5953671}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **NIVEL 1 - Análisis de Ocupación**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files\npath = '/kaggle/input/uab-the-hack-2024/'\npath_out = '/kaggle/working/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-27T09:42:32.463072Z","iopub.execute_input":"2024-10-27T09:42:32.463532Z","iopub.status.idle":"2024-10-27T09:42:32.469493Z","shell.execute_reply.started":"2024-10-27T09:42:32.463489Z","shell.execute_reply":"2024-10-27T09:42:32.468136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Limpieza y preprocesamiento de datos","metadata":{}},{"cell_type":"markdown","source":"Para llevar a cabo un análisis de ocupación preciso, es esencial trabajar con datos limpios y consistentes. Nuestro conjunto de datos está dividido en cinco archivos principales: `caracteristicas.csv`, `recursos_caracteristicas.csv`, `ubicaciones.csv`, `grupos.csv`, y `calendario_grupos.csv`. Cada dataset contiene información crítica sobre las características, recursos, ubicaciones y horarios de los diferentes grupos.\n\nLa limpieza de datos implica revisar y transformar estos archivos para asegurar su consistencia, detectar y manejar valores nulos, corregir posibles errores en los datos y estandarizar formatos. Este proceso permitirá obtener una base de datos confiable y optimizada para la fase de análisis, minimizando errores y mejorando la precisión de los resultados. Utilizaremos bibliotecas de Python como `pandas` y `numpy` para realizar esta tarea de manera eficiente y estructurada.","metadata":{}},{"cell_type":"markdown","source":"### 1. CARACTERISTICAS DATASET","metadata":{}},{"cell_type":"code","source":"carac = pd.read_csv(path + 'caracteristicas.csv', sep=',', low_memory=False)\n\n# Count lines\nprint(\"Count lines: \", carac.shape[0])\n\n# Ordenar por 'ID_CARACTERISTICA' para organizar los datos\ncarac = carac.sort_values(by='ID_CARACTERISTICA')\ncarac.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:42:32.477681Z","iopub.execute_input":"2024-10-27T09:42:32.478089Z","iopub.status.idle":"2024-10-27T09:42:32.504261Z","shell.execute_reply.started":"2024-10-27T09:42:32.478049Z","shell.execute_reply":"2024-10-27T09:42:32.503086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para comenzar el análisis de ocupación, es importante asegurarse de que los datos de caracteristicas.csv sean consistentes y libres de valores que puedan introducir errores. Este proceso de limpieza incluye:\n\n1. Ordenar los Datos: Ordenamos por la columna ID_CARACTERISTICA para mejorar la organización y facilitar el acceso a los registros.\n2. Reemplazo de Valores Inconsistentes: La columna QL_MAGNITUD_CARACTERISTICA contiene valores similares en diferentes formatos (s/n, S/N, s/n.). Estos fueron reemplazados por un único valor estándar (S/N), mejorando la coherencia de los datos.\n3. Eliminación de Filas con Valores No Deseados: \n    * Filas con ID_TP_VAL_CARAC igual a -1 y con ID_CARACTERISTICA igual a -1 fueron eliminadas, ya que estos valores son inadecuados o indicativos de datos faltantes.\n    * Filas en la columna ID_SN_BAIXA_TCARAC que contienen el valor S, que indica elementos dados de baja, también fueron excluidas.\n    * La columna ID_DATA_BAIXA_TCARAC fue eliminada tras filtrar filas que contenían datos en ella, ya que estos registros son irrelevantes para el análisis.\n4. Eliminación de Columnas Irrelevantes: Se eliminan las columnas QL_MAGNITUD_CARACTERISTICA, ID_SN_DUPLICAR_RECURS, ID_SN_BAIXA_TCARAC, y ID_DATA_BAIXA_TCARAC, ya que no aportan información adicional para este análisis.","metadata":{}},{"cell_type":"code","source":"# Estandarización de valores en 'QL_MAGNITUD_CARACTERISTICA': reemplazar variantes de 'S/N' por un valor consistente\ncarac['QL_MAGNITUD_CARACTERISTICA'] = carac['QL_MAGNITUD_CARACTERISTICA'].replace(['s/n', 's/n.'], 'S/N')\n\n# Filtrar filas con valores no deseados\n# Eliminar filas con -1 en 'ID_TP_VAL_CARAC' o 'ID_CARACTERISTICA'\ncarac = carac[(carac['ID_TP_VAL_CARAC'] != -1) & (carac['ID_CARACTERISTICA'] != -1)]\n\n# Eliminar filas marcadas como bajas (valor 'S' en 'ID_SN_BAIXA_TCARAC')\ncarac = carac[carac['ID_SN_BAIXA_TCARAC'] != 'S']\n\n# Eliminar columnas irrelevantes\ncarac = carac.drop(columns=['QL_MAGNITUD_CARACTERISTICA', 'ID_SN_DUPLICAR_RECURS', 'ID_SN_BAIXA_TCARAC'])\n\n# Filtrar filas con valores nulos en 'ID_DATA_BAIXA_TCARAC' y eliminar la columna\ncarac = carac[carac['ID_DATA_BAIXA_TCARAC'].isnull()]\ncarac = carac.drop(columns=['ID_DATA_BAIXA_TCARAC'])\n\n# Mostrar valores únicos y conteos para cada columna\nfor col in carac.columns:\n    #print(f\"* {col} : {carac[col].unique()}\")\n    print(col)\n    #print(carac[col].value_counts())\n    carac[col].value_counts().plot(kind='bar', title=f\"Frecuencia de valores en {col}\", figsize=(20, 10))\n    plt.xlabel(col)\n    plt.ylabel(\"Frecuencia\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:42:32.506398Z","iopub.execute_input":"2024-10-27T09:42:32.506842Z","iopub.status.idle":"2024-10-27T09:42:35.274470Z","shell.execute_reply.started":"2024-10-27T09:42:32.506794Z","shell.execute_reply":"2024-10-27T09:42:35.273173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vista previa del dataset limpio\ncarac.head()\n\n# Guardar dataset del output\ncarac.to_csv(path_out + '/caracteristicas.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:42:35.276377Z","iopub.execute_input":"2024-10-27T09:42:35.276859Z","iopub.status.idle":"2024-10-27T09:42:35.284494Z","shell.execute_reply.started":"2024-10-27T09:42:35.276808Z","shell.execute_reply":"2024-10-27T09:42:35.283412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La salida incluye gráficos de barras para cada columna, mostrando la frecuencia de valores únicos. Aquí algunos puntos clave observados:\n\n* ID_CARACTERISTICA: Se identificaron 76 valores únicos, cada uno representando una característica específica. La distribución es equilibrada, ya que cada ID_CARACTERISTICA aparece una vez, indicando un conjunto diverso de características.\n* DS_CARACTERISTICA: Esta columna contiene descripciones en catalán para cada característica. Al examinar los valores únicos, observamos que hay una amplia variedad de recursos documentados, desde Impressora matricial hasta Capacitat del grup.\n* QL_MEMO_CARACTERISTICA: Valores únicos que actúan como identificadores de características específicas (como Codi divisió, Canó, etc.). La distribución aquí también es uniforme, lo que muestra que cada característica es única y específica.\n* ID_TP_VAL_CARAC: Los valores más frecuentes son B (55 ocurrencias), seguido de E, L, y C. Esto puede indicar tipos o clasificaciones de características dentro de la base de datos.","metadata":{}},{"cell_type":"markdown","source":"### 2. RECURSOS CARACTERISTICAS DATASET","metadata":{}},{"cell_type":"code","source":"rec_carac = pd.read_csv(path + 'recursos_caracteristicas.csv', sep=',', low_memory=False)\n\n# Count lines\nprint(\"Count lines: \", rec_carac.shape[0])\n\n# Ordenar el DataFrame por la columna ID_RECURS\nrec_carac = rec_carac.sort_values(by='ID_RECURS')\nrec_carac.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:42:35.287033Z","iopub.execute_input":"2024-10-27T09:42:35.287435Z","iopub.status.idle":"2024-10-27T09:42:35.655063Z","shell.execute_reply.started":"2024-10-27T09:42:35.287393Z","shell.execute_reply":"2024-10-27T09:42:35.653948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El dataset recursos_caracteristicas.csv, es esencial garantizar que los datos sean consistentes y estén libres de valores que puedan introducir errores. Los pasos de limpieza de datos incluyen:\n\n1. Ordenación de los Datos: Los datos se ordenaron por la columna ID_RECURS, mejorando la organización y facilitando el acceso a los registros para el análisis.\n2. Eliminación de Columnas Irrelevantes: Se eliminaron columnas como ID_DATA_VIG_INI_VALOR, ID_DATA_VIG_FIN_VALOR, IND_VALOR_CARAC_ALFANUMERIC, IND_VALOR_CARAC_NUMERIC, e IND_PCT_REC_CENTRE. Estas columnas no aportan información relevante para nuestro objetivo actual y su eliminación simplifica el dataset, permitiendo un análisis más preciso y eficiente.\n3. Filtrado de Filas con Valores No Deseados: Se eliminaron las filas que contenían el valor -1 en cualquiera de sus columnas. Este valor puede indicar datos no válidos o ausentes y su presencia podría distorsionar el análisis.\n4. Manejo de Datos Faltantes: Después de filtrar los datos no deseados, se eliminaron las filas con valores faltantes (NaN). Esta limpieza adicional asegura que el análisis solo utilice datos completos y precisos.\n5. Exploración y Visualización de Frecuencia de Valores: Para cada columna en el dataset, se realizó un análisis de los valores únicos y su frecuencia. Se generaron gráficos de barras para visualizar la distribución de los datos en cada columna, lo cual permite identificar rápidamente cualquier irregularidad o patrón en los datos.\n","metadata":{}},{"cell_type":"code","source":"# Eliminar columnas innecesarias\nrec_carac = rec_carac.drop(columns=[\n    'ID_DATA_VIG_INI_VALOR', \n    'ID_DATA_VIG_FIN_VALOR', \n    'IND_VALOR_CARAC_ALFANUMERIC', \n    'IND_VALOR_CARAC_NUMERIC', \n    'IND_PCT_REC_CENTRE'\n])\n\n# Filtrar filas con valores -1\nrec_carac = rec_carac[rec_carac != -1].dropna()\n\n# Imprimir los valores únicos y las frecuencias de cada columna\nfor col in rec_carac:\n    #print(f\"* {col}: {rec_carac[col].unique()}\")\n    print(col)\n    #print(rec_carac[col].value_counts())\n    \n    # Graficar las frecuencias de los valores\n    rec_carac[col].value_counts().plot(kind='bar', figsize=(20, 10))\n    plt.show()\n\nrec_carac.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:42:35.656281Z","iopub.execute_input":"2024-10-27T09:42:35.656660Z","iopub.status.idle":"2024-10-27T09:44:19.502060Z","shell.execute_reply.started":"2024-10-27T09:42:35.656622Z","shell.execute_reply":"2024-10-27T09:44:19.500841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vista previa del dataset limpio\nrec_carac.head()\n\n# Guardar dataset del output\nrec_carac.to_csv(path_out + '/recursos_caracteristicas.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:19.503548Z","iopub.execute_input":"2024-10-27T09:44:19.503883Z","iopub.status.idle":"2024-10-27T09:44:20.126111Z","shell.execute_reply.started":"2024-10-27T09:44:19.503849Z","shell.execute_reply":"2024-10-27T09:44:20.124972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En esta etapa, se realiza una verificación final para asegurar la calidad de los datos antes de integrarlos. Primero, se examinan los valores NaN en cada columna de los datasets carac y rec_carac para confirmar que no existan valores faltantes que puedan afectar el análisis. Posteriormente, ambos datasets se combinan utilizando la columna ID_CARACTERISTICA, generando un único dataset que contiene toda la información relevante sobre características y recursos. Esta integración facilitará el análisis posterior y garantizará que se cuente con un conjunto de datos completo y limpio.","metadata":{}},{"cell_type":"code","source":"# Verificar valores NaN en cada columna de ambos datasets para asegurar una correcta limpieza de datos\nprint(\"Valores nulos en carac:\\n\", carac.isnull().sum())\nprint(\"Valores nulos en rec_carac:\\n\", rec_carac.isnull().sum())\n\n# Unir los datasets carac y rec_carac usando la columna ID_CARACTERISTICA como clave\nmerged_data = pd.merge(carac, rec_carac, on='ID_CARACTERISTICA', how='inner')\n\n# Crear la ruta de guardado del dataset combinado\noutput_path = os.path.join(path_out, 'merged_caracteristicas_recursos.csv')\n\n# Guardar el dataset combinado en un archivo CSV\nmerged_data.to_csv(output_path, index=False)\nprint(f\"Dataset combinado guardado en: {output_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:20.127592Z","iopub.execute_input":"2024-10-27T09:44:20.128001Z","iopub.status.idle":"2024-10-27T09:44:20.973476Z","shell.execute_reply.started":"2024-10-27T09:44:20.127960Z","shell.execute_reply":"2024-10-27T09:44:20.972104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este apartado, se analiza el conjunto de datos combinado en el archivo merged_caracteristicas_recursos.csv, el cual contiene información relevante sobre características y recursos. Primero, se carga el dataset y se contabilizan el número total de registros. A continuación, se filtran los datos para incluir únicamente aquellos correspondientes al *ID_CENTRE_ADMINISTRADOR 115*, que representa a la *Escuela de Ingeniería*.\n\nLuego, se realiza un conteo de las ocurrencias de cada característica, lo que permite identificar las más y menos comunes dentro de este contexto específico. Para facilitar la interpretación de los resultados, se generan gráficos de barras que visualizan la distribución de las características en el Centro 115. Esta etapa es crucial para comprender la información y orientar análisis futuros.","metadata":{}},{"cell_type":"code","source":"# Cargar los datos combinados\nmerged_data = pd.read_csv(path_out + 'merged_caracteristicas_recursos.csv', sep=',')\n\n# Contar el número de líneas en el DataFrame\nprint(\"Número total de líneas: \", merged_data.shape[0])\n\n# Filtrar las filas donde ID_CENTRE_ADMINISTRADOR es igual a 115\nmerged_data = merged_data[merged_data['ID_CENTRE_ADMINISTRADOR'] == 115]\n\n# Contar el número de líneas después de aplicar el filtro\nprint(\"Número de líneas en la Escuela de Ingeniería: \", merged_data.shape[0])\n\n# Contar el número de ocurrencias de cada DS_CARACTERISTICA y ordenar por conteo\nmd = merged_data['DS_CARACTERISTICA'].value_counts().sort_values(ascending=True)\n\n# Imprimir los conteos para verificación\nprint(md)\n\n# Imprimir el conteo de cada característica\n'''for i in md.index:\n    print(i, \":\", md[i])\n    print(merged_data[merged_data['DS_CARACTERISTICA'] == i].head(1))\n'''\n# Graficar el conteo de valores de DS_CARACTERISTICA\nmd.plot(kind='bar', figsize=(20, 10))\nplt.show()\n\n# Convertir la Serie a un DataFrame y restablecer el índice para incluir DS_CARACTERISTICA\nmd_df = md.reset_index()\nmd_df.columns = ['DS_CARACTERISTICA', 'Count']  # Renombrar las columnas\n\n# Crear la ruta de salida para el nuevo archivo CSV\noutput_path = os.path.join(path_out, 'ee_caracteristicas_recursos.csv')\n\n# Guardar el DataFrame en un archivo CSV\nmd_df.to_csv(output_path, index=False)\nprint(f\"Dataset de la Escuela de Ingeniería guardado en: {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:20.974831Z","iopub.execute_input":"2024-10-27T09:44:20.975189Z","iopub.status.idle":"2024-10-27T09:44:21.973866Z","shell.execute_reply.started":"2024-10-27T09:44:20.975155Z","shell.execute_reply":"2024-10-27T09:44:21.972745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. UBICACIONES DATASET","metadata":{}},{"cell_type":"code","source":"# Cargar el dataset de ubicaciones\nubi = pd.read_csv(path + 'ubicaciones.csv', sep=',')\nubi.head()  # Mostrar las primeras filas del dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:21.975972Z","iopub.execute_input":"2024-10-27T09:44:21.976455Z","iopub.status.idle":"2024-10-27T09:44:22.012630Z","shell.execute_reply.started":"2024-10-27T09:44:21.976406Z","shell.execute_reply":"2024-10-27T09:44:22.011579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Este código se centra en la limpieza, filtrado y análisis de un conjunto de datos sobre ubicaciones en un campus, con el objetivo de obtener información relevante sobre los espacios disponibles. A continuación se describen las etapas clave del proceso:\n\n1. Limpieza de Datos:\n    * Se eliminan las filas con valores nulos del DataFrame para garantizar que las operaciones posteriores se realicen sobre datos completos y válidos.\n    * Los nombres de las ubicaciones se normalizan a minúsculas, lo que ayuda a prevenir inconsistencias y facilita la búsqueda y comparación de datos.\n2. Eliminación de Columnas Irrelevantes:\n    Se elimina la columna QL_PORTA, ya que no es necesaria para el análisis, simplificando así el conjunto de datos.\n3. Definición de Términos de Inclusión y Exclusión:\n    Se establecen listas de términos que se quieren incluir (por ejemplo, 'lab.', 'laboratori', 'aula', 'seminaris') y otros que se desean excluir (por ejemplo, 'accés', 'despatx', 'magatzem'). Esto permite filtrar las ubicaciones y concentrarse en aquellas de interés.\n4. Filtrado de Datos:\n    Se utiliza un patrón basado en las listas de inclusión y exclusión para filtrar el DataFrame, quedándose únicamente con las ubicaciones que contienen términos relevantes. Este filtrado asegura que solo se consideren espacios como laboratorios y aulas, mientras que se eliminan oficinas y otros espacios no pertinentes.\n5. Consolidación de Filas:\n    Se concatenan las filas de los edificios específicos (Q, C y B) en un solo DataFrame, facilitando así un análisis más claro de las ubicaciones dentro de esos edificios.\n6. Visualización y Análisis:\n    * Se cuenta el número de ubicaciones marcadas como \"bajas\" (no en uso) y se visualiza esta información a través de un gráfico de barras. Esto proporciona una representación visual clara de cuántas ubicaciones están fuera de uso.\n    * A continuación, se filtran las ubicaciones para excluir aquellas marcadas como bajas y se elimina la columna correspondiente, ya que no es necesaria para el análisis posterior.\n7. Conteo de Ubicaciones por Edificio:\n    * Finalmente, se cuenta la cantidad de ubicaciones por edificio y se genera otro gráfico de barras que muestra la distribución de ubicaciones en los edificios considerados, lo que permite visualizar de forma clara la disponibilidad de espacios en el campus.\nEste proceso en su conjunto busca proporcionar un conjunto de datos claro y útil para la toma de decisiones sobre el uso y la gestión de los espacios en el campus, centrándose en la relevancia de las ubicaciones educativas y de trabajo.","metadata":{}},{"cell_type":"code","source":"# Limpiar el dataframe eliminando filas con valores nulos\nubi = ubi.dropna(axis=0, how='any')\n\n# Normalizar los nombres de las ubicaciones a minúsculas\nubi['DS_UBICACIO'] = ubi['DS_UBICACIO'].str.lower()\n\n# Eliminar la columna QL_PORTA\nubi.drop(columns=['QL_PORTA'], inplace=True)\n\n# Definir listas de inclusiones y exclusiones\ninclusiones = ['lab.', 'laboratori', 'aula', 'seminaris']\nexclusiones = ['accés', 'despatx', 'magatzem', 'passadís', 'lavabos', 'labavos', 'investigació']\n\n# Crear patrones para las inclusiones y exclusiones\np_inc = \"|\".join(inclusiones)\np_excl = \"|\".join(exclusiones)\n\n# Filtrar el dataframe para incluir solo las ubicaciones relevantes\nubi_fin = ubi[ubi['DS_UBICACIO'].str.contains(p_inc)]\nubi_fin = ubi_fin[~ubi_fin['DS_UBICACIO'].str.contains(p_excl, case=False, na=False)]\n\n# Concatenar filas específicas de edificios\nubi_fin = pd.concat([ubi_fin[ubi_fin['ID_EDIFICI'] == 'Q'], \n                      ubi_fin[ubi_fin['ID_EDIFICI'] == 'C'], \n                      ubi_fin[ubi_fin['ID_EDIFICI'] == 'B']], \n                     ignore_index=True)\n\n# Reiniciar el índice del dataframe final\nubi_fin.reset_index(drop=True, inplace=True)\n\n# Mostrar las primeras filas del dataframe final\nprint(ubi_fin.head())\n\n# Contar la cantidad de valores en ID_SN_BAIXA\nbaixa_count = ubi_fin['ID_SN_BAIXA'].value_counts()\n\n# Graficar los conteos de ID_SN_BAIXA\nplt.figure(figsize=(10, 5))  # Ajustar el tamaño de la figura\nplt.bar(baixa_count.index, baixa_count.values)\nplt.ylabel('Cantidad')\nplt.xlabel('Valor')\nplt.title('Conteo de Baixas por Valor')\nplt.xticks(rotation=0)  # Rotar etiquetas en el eje x si es necesario\nplt.show()\n\n# Filtrar el dataframe final para excluir valores de baja\nubi_fin = ubi_fin[ubi_fin['ID_SN_BAIXA'] == 'N']\n\n# Eliminar la columna ID_SN_BAIXA\nubi_fin.drop(columns=['ID_SN_BAIXA'], inplace=True)\n\n# Reiniciar el índice del dataframe final después de la eliminación\nubi_fin.reset_index(drop=True, inplace=True)\n\n# Visualizar el dataframe final\n#print(ubi_fin.head())\n\n# Graficar el conteo de ubicaciones por edificio\nedificio_count = ubi_fin['ID_EDIFICI'].value_counts()\n\n# Crear un gráfico de barras para los edificios\nplt.figure(figsize=(10, 5))  # Ajustar el tamaño de la figura\nplt.bar(edificio_count.index, edificio_count.values)\nplt.ylabel('Cantidad de Ubicaciones')\nplt.xlabel('ID del Edificio')\nplt.title('Conteo de Ubicaciones por Edificio')\nplt.xticks(rotation=0)  # Rotar etiquetas en el eje x si es necesario\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:22.016972Z","iopub.execute_input":"2024-10-27T09:44:22.017438Z","iopub.status.idle":"2024-10-27T09:44:22.540883Z","shell.execute_reply.started":"2024-10-27T09:44:22.017391Z","shell.execute_reply":"2024-10-27T09:44:22.539747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ubicaciones.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:22.542217Z","iopub.execute_input":"2024-10-27T09:44:22.542619Z","iopub.status.idle":"2024-10-27T09:44:22.558157Z","shell.execute_reply.started":"2024-10-27T09:44:22.542580Z","shell.execute_reply":"2024-10-27T09:44:22.556918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Este bloque de código tiene como objetivo verificar la calidad de los datos en el dataset de *ubicaciones*. Al imprimir la cantidad de valores nulos en cada columna del dataframe, se garantiza que se identifiquen posibles problemas de limpieza antes de realizar análisis posteriores. Esta verificación es crucial para asegurar que los análisis futuros sean precisos y no se vean afectados por datos faltantes.","metadata":{}},{"cell_type":"code","source":"print(\"Valores nulos en ubicaciones:\\n\", ubi.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:22.559303Z","iopub.execute_input":"2024-10-27T09:44:22.559684Z","iopub.status.idle":"2024-10-27T09:44:22.574679Z","shell.execute_reply.started":"2024-10-27T09:44:22.559651Z","shell.execute_reply":"2024-10-27T09:44:22.573306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se considera que cada estudiante necesita aproximadamente 1.5 m². A continuación se describen los pasos clave del proceso:\n\n1. Definición de Variables:\n    * Se extraen dos columnas del DataFrame df_fin: ID_TIPUS_DEPENDENCIA, que indica el tipo de dependencia o aula, y IND_METRES, que representa la superficie del aula en metros cuadrados.\n    * Se define superficie_alumno como 1.5 m², que es el espacio que se estima que ocupa un alumno.\n2. Estimación de Capacidad de Aulas:\n    * Se inicializa una lista vacía llamada capacidad_aulas para almacenar las capacidades calculadas.\n    * Se itera sobre cada fila del DataFrame usando un bucle for. Para cada aula, se evalúa el valor de ID_TIPUS_DEPENDENCIA:\n        - Si el valor es menor que 10, se calcula la capacidad del aula dividiendo IND_METRES entre superficie_alumno. Este cálculo se redondea hacia abajo usando math.floor() para obtener un número entero de alumnos que pueden ocupar el aula.\n        - Si el valor de ID_TIPUS_DEPENDENCIA es 10 o mayor, se asume que la capacidad ya está correctamente registrada, y se agrega directamente a la lista capacidad_aulas.\n3. Actualización del DataFrame:\n    * Se eliminan las columnas ID_TIPUS_DEPENDENCIA e IND_METRES del DataFrame, ya que ya no son necesarias después de realizar los cálculos.\n    * Se añade una nueva columna al DataFrame llamada CAPACIDAD, que contiene los valores estimados o los valores originales de capacidad según corresponda.\n4. Exportación del DataFrame:\n    Finalmente, el DataFrame df_fin, que ahora incluye la capacidad estimada de las aulas, se guarda en un nuevo archivo CSV en la ruta especificada \"ubicaciones_cleaned.csv\". Este archivo contiene la información actualizada sobre las ubicaciones, lista para su análisis o uso posterior.","metadata":{}},{"cell_type":"code","source":"# Definimos el espacio que ocupa un alumno en metros cuadrados\nsuperficie_alumno = 1.5\n\n# Calculamos la capacidad estimada de las aulas\n# Usamos np.where para asignar la capacidad según las condiciones especificadas\nubi_fin['CAPACIDAD'] = np.where(\n    ubi_fin['ID_TIPUS_DEPENDENCIA'] < 10,\n    np.floor(ubi_fin['IND_METRES'] / superficie_alumno),  # Capacidad estimada\n    ubi_fin['ID_TIPUS_DEPENDENCIA']  # Usar la capacidad original\n)\n\n# Eliminar columnas que ya no son necesarias\nubi_fin.drop(columns=['ID_TIPUS_DEPENDENCIA', 'IND_METRES'], inplace=True)\n\n# Guardar el DataFrame actualizado en un archivo CSV\noutput_path = path_out + 'ubicaciones_cleaned.csv'\nubi_fin.to_csv(output_path, index=False)\nprint(f\"Dataset de la Escuela de Ingeniería guardado en: {output_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:22.576183Z","iopub.execute_input":"2024-10-27T09:44:22.576602Z","iopub.status.idle":"2024-10-27T09:44:22.591944Z","shell.execute_reply.started":"2024-10-27T09:44:22.576562Z","shell.execute_reply":"2024-10-27T09:44:22.590737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. GRUPOS DATASET","metadata":{}},{"cell_type":"code","source":"# Cargar el dataset de ubicaciones\ngrupos = pd.read_csv(path + 'grupos.csv', sep=',', low_memory=False)\n\n# Filtrar datos para el año académico 2024\ngrupos = grupos[grupos['ID_CURSO_ACADEMICO'] == 2024].drop(columns=['ID_SEMESTRE'])\n\n# Count lines\nprint(\"Count lines: \", grupos.shape[0])\n\n# Ordenar el DataFrame por la columna ID_RECURS\ngrupos = grupos.sort_values(by='ID_GRUPO')\ngrupos.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:22.593529Z","iopub.execute_input":"2024-10-27T09:44:22.594093Z","iopub.status.idle":"2024-10-27T09:44:24.008660Z","shell.execute_reply.started":"2024-10-27T09:44:22.594040Z","shell.execute_reply":"2024-10-27T09:44:24.007355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este apartado, realizamos la limpieza del conjunto de datos correspondiente al año académico 2024. A través de diversos pasos, filtramos, codificamos y eliminamos columnas innecesarias para obtener un dataset más manejable y relevante.\n\n1. Filtrado de Datos: Seleccionamos solo las filas correspondientes al año académico 2024 y eliminamos columnas que no aportan información útil, como ID_SEMESTRE.\n2. Codificación de Variables: Transformamos la variable ID_PERIODO_DOCENTE en valores numéricos para simplificar el análisis y facilitar el uso de técnicas estadísticas.\n3. Filtrado Adicional: Mantuvimos únicamente los grupos con un número positivo de alumnos y horas previstas, asegurando que nos enfocamos en grupos operativos.","metadata":{}},{"cell_type":"code","source":"# Imprimir el número de entradas y semestres únicos\nprint(f'Número de entradas: {len(grupos)}')\nprint(f'Número de semestres: {grupos[\"ID_PERIODO_DOCENTE\"].nunique()}')\n\n# Codificar ID_PERIODO_DOCENTE en valores numéricos\ngrupos['ID_PERIODO_DOCENTE'] = pd.Categorical(grupos['ID_PERIODO_DOCENTE']).codes\n\n# Contar grupos por periodo docente\ngrupo_counts = grupos.groupby('ID_PERIODO_DOCENTE').size()\nprint(grupo_counts)\n\n# Comprobar existencia del grupo ID -1\ngroup_id = -1\nif group_id in grupos['ID_GRUPO'].values:\n    print(f'El grupo ID {group_id} existe en el conjunto de datos.')\nelse:\n    print(f'El grupo ID {group_id} no existe en el conjunto de datos.')\n\n# Eliminar columnas innecesarias\ncolumns_to_drop = [\n    \"IDIOMA_MAJORITARI\", \n    \"ID_IDIOMA\", \n    \"ID_UNIDAD_DOCENTE\", \n    \"ID_DEPARTAMENT_DELEG_DECODA\", \n    \"ID_VALOR_PERIODO_DOC\", \n    \"ID_CENTRO_DEPART\"\n]\ngrupos.drop(columns=columns_to_drop, inplace=True)\n\n# Imprimir el número de entradas restantes y los tipos de docencia únicos\nprint(f'Número de entradas restantes: {len(grupos)}')\n#print(grupos['ID_TIPO_DOCENCIA'].value_counts())\nprint(f'Número de tipos de docencia únicos: {grupos[\"ID_TIPO_DOCENCIA\"].nunique()}')\n\n# Filtrar filas con alumnos y horas previstas mayores que cero\ngrupos = grupos[(grupos['IND_ALUMNOS_GRUPO_REAL'] > 0) & (grupos['IND_HORAS_PREVISTAS'] > 0)]\n\n# Gráfico de la distribución de alumnos por grupo\nplt.figure(figsize=(12, 6))\nsns.histplot(grupos['IND_ALUMNOS_GRUPO_REAL'], bins=30, kde=True)\nplt.title('Distribución de Alumnos por Grupo')\nplt.xlabel('Número de Alumnos')\nplt.ylabel('Frecuencia')\nplt.grid()\nplt.show()\n\n# Gráfico de grupos por tipo de docencia\nplt.figure(figsize=(12, 6))\ntipo_docencia_counts = grupos['ID_TIPO_DOCENCIA'].value_counts()\nsns.barplot(x=tipo_docencia_counts.index, y=tipo_docencia_counts.values)\nplt.title('Cantidad de Grupos por Tipo de Docencia')\nplt.xlabel('Tipo de Docencia')\nplt.ylabel('Número de Grupos')\nplt.xticks(rotation=45)\nplt.grid()\nplt.show()\n\n# Guardar el conjunto de datos limpio en un archivo CSV\ngrupos.to_csv('grupos_cleaned.csv', index=False)\n\n# Nota sobre grupos eliminados si no tienen entradas en el horario\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:24.010074Z","iopub.execute_input":"2024-10-27T09:44:24.010537Z","iopub.status.idle":"2024-10-27T09:44:25.097402Z","shell.execute_reply.started":"2024-10-27T09:44:24.010495Z","shell.execute_reply":"2024-10-27T09:44:25.096268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. CALENDARIO GRUPOS DATASET","metadata":{}},{"cell_type":"code","source":"# Cargar el conjunto de datos para su análisis\ncalendario_grupos = pd.read_csv(path + 'calendario_grupos.csv', sep=',', low_memory=False)\n# Mostrar las primeras filas del conjunto de datos para entender su estructura\ncalendario_grupos.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:25.098825Z","iopub.execute_input":"2024-10-27T09:44:25.099193Z","iopub.status.idle":"2024-10-27T09:44:28.271379Z","shell.execute_reply.started":"2024-10-27T09:44:25.099153Z","shell.execute_reply":"2024-10-27T09:44:28.269995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este apartado, hemos llevado a cabo una serie de pasos para cargar y limpiar el conjunto de datos correspondiente a los grupos en el calendario académico. Primero, hemos importado el archivo CSV y visualizado las primeras filas para entender su estructura. A continuación, filtramos las ubicaciones que contienen la letra 'Q', lo que nos permite enfocarnos en las áreas de interés.\n\nSe eliminan varias columnas que no aportan información relevante al análisis, lo que facilita la manipulación y comprensión de los datos. Además, se filtran los grupos que tienen un ID inválido para asegurar que trabajamos solo con información confiable.\n\nFinalmente, se eliminan los duplicados y se guarda el conjunto de datos limpio en un nuevo archivo CSV.","metadata":{}},{"cell_type":"code","source":"# Filtrar el conjunto de datos para incluir solo las ubicaciones que contienen 'Q'\n#calendario_grupos = calendario_grupos[calendario_grupos['ID_UBICACION'].str.contains(r'Q', regex=True, na=False)]\n\n# Verificar los valores únicos en la columna 'ID_ORIGEN'\nprint(calendario_grupos['ID_ORIGEN'].unique())\n\n# Eliminar columnas que no son relevantes para nuestro análisis\ncolumns_to_drop = [\n    'ID_ORIGEN', 'IND_HORAS_CALENDARIO', 'IND_PCT_CLAU', \n    'ID_UNITAT_COST', 'ID_PROJECTE', 'ID_CLAU_ACCES', \n    'ID_TIPO_RESERVA', 'ID_CENTRE_COST', 'ID_QUART_HORA', \n    'ID_SESION', 'IND_METRES', 'ID_CURSO_ACADEMICO', \n    'ID_RESPONSABLE_RESERVA', 'ID_UBICACION_SAMAS', 'ID_UBICACION'\n]\ncalendario_grupos = calendario_grupos.drop(columns=columns_to_drop)\n\n# Filtrar grupos que tienen un ID válido\ncalendario_grupos = calendario_grupos[calendario_grupos['ID_GRUPO'] != \"-1\"]\n\n# Eliminar duplicados\ncalendario_grupos = calendario_grupos.drop_duplicates()\n\n# Guardar el conjunto de datos limpio en un archivo CSV\ncalendario_grupos.to_csv(path_out + 'calendario_grupos_clean.csv', index=False)\n\n# Mostrar el conjunto de datos limpio\nprint(calendario_grupos.head())\n\n# Crear un gráfico de barras que muestre la distribución de grupos por ID_UBICACION\nplt.figure(figsize=(12, 6))\ncalendario_grupos['ID_GRUPO'].value_counts().plot(kind='bar')\nplt.title('Distribución de Grupos')\nplt.tight_layout()\nplt.show()\n\nprint(calendario_grupos.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:44:28.272711Z","iopub.execute_input":"2024-10-27T09:44:28.273064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identificación de ineficiencias","metadata":{}},{"cell_type":"markdown","source":"## Visualización de patrones de ocupación","metadata":{}}]}